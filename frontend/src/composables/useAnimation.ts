import { ref, Ref, nextTick } from 'vue';

export interface AnimationState {
  isProcessing: Ref<boolean>;
  audioUrl: Ref<string>;
  animationTimer: Ref<number | null>;
  visemeTimeline: Array<{ id: number; t: number }>;
  onAnimate: () => Promise<void>;
  startTimelineAnimation: (audio: HTMLAudioElement) => void;
  handleViseme: (id: number, t: number, anim?: string) => void;
  syncVisemeWithAudio: (audio: HTMLAudioElement) => void;
  speak: () => Promise<void>;
}

export function useAnimation(
  text: Ref<string>,
  ssml: Ref<string>,
  selectedVoice: Ref<string>,
  modelViewer: any,
  currentAction: Ref<string>,
  currentEmotion: Ref<string>,
  synthesizeSpeech: (content: string, voice: string, isSSML: boolean, handleViseme?: (id: number, t: number, anim?: string) => void) => Promise<Blob>,
  t: (key: string) => string,
  actionKeyframes?: Ref<any[]>,
  emotionKeyframes?: Ref<any[]>,
  audioPlayer?: Ref<HTMLAudioElement | null>
): AnimationState {
  const isProcessing = ref(false);
  const audioUrl = ref<string>('');
  const animationTimer = ref<number | null>(null);
  const visemeTimeline: Array<{ id: number; t: number }> = [];

  // ä¸»è¦åŠ¨ç”»ç”Ÿæˆå‡½æ•°
  async function onAnimate() {
    if (!text.value.trim()) {
      alert(t('animate.textRequired'));
      return;
    }

    if (text.value.length > 180) {
      alert(t('animate.textTooLong'));
      return;
    }

    console.log('ğŸ¬ Starting animation generation...');
    console.log('Text:', text.value);
    console.log('SSML:', ssml.value);
    console.log('Selected voice:', selectedVoice.value);
    console.log('Model viewer:', modelViewer.value);

    try {
      isProcessing.value = true;
      console.log('ğŸ¯ isProcessing set to true');
      
      // æ¸…ç†ä¹‹å‰çš„çŠ¶æ€
      if (animationTimer.value) {
        console.log('ğŸ§¹ Cleaning up previous animation timer');
        clearInterval(animationTimer.value);
        animationTimer.value = null;
      }
      
      // åœæ­¢å½“å‰éŸ³é¢‘æ’­æ”¾
      const currentAudio = audioPlayer?.value || document.querySelector('audio') as HTMLAudioElement;
      if (currentAudio) {
        currentAudio.pause();
        currentAudio.currentTime = 0;
      }
      
      // é‡ç½®æ¨¡å‹çŠ¶æ€
      if (modelViewer.value) {
        console.log('ğŸ”„ Resetting model state');
        modelViewer.value.playAnimation('Idle');
        modelViewer.value.updateEmotion('');
      }
      
      console.log('ğŸ”Š Synthesizing speech...');
      const audioBlob = await synthesizeSpeech(
        ssml.value || text.value,
        selectedVoice.value,
        Boolean(ssml.value),
        handleViseme,
      );
      console.log('âœ… Speech synthesized successfully');
      
      // æ¸…ç†ä¹‹å‰çš„éŸ³é¢‘ URL
      if (audioUrl.value) {
        try {
          URL.revokeObjectURL(audioUrl.value);
        } catch (error) {
          console.warn('âš ï¸ Failed to revoke previous audio URL:', error);
        }
      }
      
      try {
        audioUrl.value = URL.createObjectURL(audioBlob);
        console.log('âœ… Audio URL created successfully:', audioUrl.value);
      } catch (error) {
        console.error('âŒ Failed to create audio URL:', error);
        throw error;
      }

      // æ’­æ”¾éŸ³é¢‘å¹¶é©±åŠ¨åŠ¨ç”»
      nextTick(() => {
        const audio = audioPlayer?.value || document.querySelector('audio') as HTMLAudioElement;
        if (audio) {
          console.log('ğŸµ Starting audio playback and animation...');
          console.log('ğŸ” Audio element state:', {
            src: audio.src,
            readyState: audio.readyState,
            networkState: audio.networkState,
            currentTime: audio.currentTime,
            duration: audio.duration || 'unknown'
          });
          
          // ç¡®ä¿éŸ³é¢‘æºå·²è®¾ç½®
          if (!audio.src && audioUrl.value) {
            console.log('ğŸ”„ Setting audio src:', audioUrl.value);
            audio.src = audioUrl.value;
          }
          
          // ç­‰å¾…éŸ³é¢‘å‡†å¤‡å°±ç»ª
          const playAudio = () => {
            audio.currentTime = 0;
            audio.play().then(() => {
              console.log('âœ… Audio started playing');
              startTimelineAnimation(audio);

              // å¼€å§‹å£å‹åŒæ­¥
              visemeTimeline.length = 0; // æ¸…ç©ºæ—§æ•°æ®
              syncVisemeWithAudio(audio);
            }).catch((error) => {
              console.error('âŒ Failed to play audio:', error);
              console.log('ğŸ” Audio element debug info:', {
                src: audio.src,
                readyState: audio.readyState,
                networkState: audio.networkState,
                error: audio.error
              });
            });
          };
          
          // å¦‚æœéŸ³é¢‘è¿˜æ²¡æœ‰å‡†å¤‡å¥½ï¼Œç­‰å¾…å®ƒåŠ è½½
          if (audio.readyState >= 2) {
            playAudio();
          } else {
            console.log('â³ Waiting for audio to be ready...');
            audio.addEventListener('canplay', playAudio, { once: true });
            audio.addEventListener('error', (e) => {
              console.error('âŒ Audio loading error:', e);
            }, { once: true });
            
            // è§¦å‘åŠ è½½
            audio.load();
          }
        } else {
          console.error('âŒ Audio element not found');
        }
      });
    } catch (error) {
      console.error('âŒ Failed to synthesize speech:', error);
      alert(t('animate.synthesisError'));
    } finally {
      isProcessing.value = false;
      console.log('ğŸ¯ isProcessing set to false');
    }
  }

  // å¯åŠ¨æ—¶é—´è½´åŠ¨ç”»
  function startTimelineAnimation(audio: HTMLAudioElement) {
    console.log('ğŸ­ Starting timeline animation...');
    
    let lastAction = currentAction.value;
    let lastEmotion = currentEmotion.value;

    // æ¸…ç†æ—§å®šæ—¶å™¨
    if (animationTimer.value) {
      console.log('ğŸ§¹ Cleaning up existing animation timer');
      clearInterval(animationTimer.value);
      animationTimer.value = null;
    }

    // è·å–å…³é”®å¸§æ•°æ®
    const actionFrames = actionKeyframes?.value || [];
    const emotionFrames = emotionKeyframes?.value || [];
    
    console.log(`ğŸ“Š Timeline animation data:`, {
      actionKeyframes: actionFrames.length,
      emotionKeyframes: emotionFrames.length,
      actionFramesData: actionFrames,
      emotionFramesData: emotionFrames
    });

    // å¦‚æœæ²¡æœ‰å…³é”®å¸§ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€
    if (actionFrames.length === 0) {
      console.log('No action keyframes found, using default Idle animation');
    }
    
    if (emotionFrames.length === 0) {
      console.log('No emotion keyframes found, using default emotion');
    }

    // åº”ç”¨åˆå§‹çŠ¶æ€
    let initialAction = 'Idle';
    let initialEmotion = '';
    
    // æŸ¥æ‰¾ t=0 æ—¶çš„å…³é”®å¸§æˆ–æœ€æ—©çš„å…³é”®å¸§
    const initialActionFrame = actionFrames
      .sort((a, b) => a.time - b.time)
      .find(frame => frame.time <= 0.1) || actionFrames[0];
    
    const initialEmotionFrame = emotionFrames
      .sort((a, b) => a.time - b.time)
      .find(frame => frame.time <= 0.1) || emotionFrames[0];
    
    if (initialActionFrame?.action) {
      initialAction = initialActionFrame.action;
    }
    
    if (initialEmotionFrame?.emotion) {
      initialEmotion = initialEmotionFrame.emotion;
    }
    
    console.log(`ğŸ¬ Setting initial states - Action: ${initialAction}, Emotion: ${initialEmotion}`);
    currentAction.value = initialAction;
    currentEmotion.value = initialEmotion;
    if (modelViewer.value) {
      modelViewer.value.playAnimation(initialAction);
      if (initialEmotion) {
        modelViewer.value.updateEmotion(initialEmotion);
      }
    }
    lastAction = initialAction;
    lastEmotion = initialEmotion;

    // åˆ›å»ºæŒ‰æ—¶é—´æ’åºçš„å…³é”®å¸§æ•°ç»„ç”¨äºæ£€æŸ¥
    const sortedActionFrames = [...actionFrames].sort((a, b) => a.time - b.time);
    const sortedEmotionFrames = [...emotionFrames].sort((a, b) => a.time - b.time);
    
    let actionIndex = 0;
    let emotionIndex = 0;

    animationTimer.value = window.setInterval(() => {
      const currentTime = audio.currentTime;
      
      // æ£€æŸ¥åŠ¨ä½œå…³é”®å¸§
      while (actionIndex < sortedActionFrames.length) {
        const frame = sortedActionFrames[actionIndex];
        if (currentTime >= frame.time && frame.action && frame.action !== lastAction) {
          const timeStr = typeof currentTime === 'number' && !isNaN(currentTime) ? currentTime.toFixed(2) : '0.00';
          console.log(`ğŸ¬ Triggering action keyframe at ${timeStr}s: ${frame.action}`);
          currentAction.value = frame.action;
          if (modelViewer.value) {
            modelViewer.value.playAnimation(frame.action);
          }
          lastAction = frame.action;
          actionIndex++;
        } else if (currentTime >= frame.time) {
          actionIndex++;
        } else {
          break;
        }
      }
      
      // æ£€æŸ¥è¡¨æƒ…å…³é”®å¸§
      while (emotionIndex < sortedEmotionFrames.length) {
        const frame = sortedEmotionFrames[emotionIndex];
        if (currentTime >= frame.time && frame.emotion && frame.emotion !== lastEmotion) {
          const timeStr = typeof currentTime === 'number' && !isNaN(currentTime) ? currentTime.toFixed(2) : '0.00';
          console.log(`ğŸ˜Š Triggering emotion keyframe at ${timeStr}s: ${frame.emotion}`);
          currentEmotion.value = frame.emotion;
          if (modelViewer.value) {
            modelViewer.value.updateEmotion(frame.emotion);
          }
          lastEmotion = frame.emotion;
          emotionIndex++;
        } else if (currentTime >= frame.time) {
          emotionIndex++;
        } else {
          break;
        }
      }
      
      // éŸ³é¢‘æ’­æ”¾ç»“æŸï¼Œæ¸…ç†å®šæ—¶å™¨å¹¶é‡ç½®ä¸º Idle
      if (audio.ended) {
        console.log('Audio ended, cleaning up animation timer');
        clearInterval(animationTimer.value!);
        animationTimer.value = null;
        // é‡ç½®åŠ¨ä½œå’Œè¡¨æƒ…
        currentAction.value = 'Idle';
        currentEmotion.value = '';
        if (modelViewer.value) {
          modelViewer.value.playAnimation('Idle');
          modelViewer.value.updateEmotion('');
        }
      }
    }, 100); // æ¯ 100ms æ£€æŸ¥ä¸€æ¬¡
    
    console.log('âœ… Timeline animation started with keyframe data');
  }

  // å¤„ç†å£å‹äº‹ä»¶
  function handleViseme(id: number, t: number, _anim?: string) {
    // å°†å£å‹äº‹ä»¶å‘é€ç»™ ModelViewer
    if (modelViewer.value && typeof (modelViewer.value as any).updateViseme === 'function') {
      (modelViewer.value as any).updateViseme(id);
    }

    // è®°å½•åˆ°æ—¶é—´è½´
    visemeTimeline.push({ id, t });
  }

  // æ’­æ”¾å™¨å¯åŠ¨æ—¶æ ¹æ® currentTime åŒæ­¥ viseme
  function syncVisemeWithAudio(audio: HTMLAudioElement) {
    let idx = 0;
    function tick() {
      const now = audio.currentTime * 1000; // ms
      while (idx < visemeTimeline.length && now >= visemeTimeline[idx].t) {
        const { id, t } = visemeTimeline[idx++];
        const timeStr = typeof now === 'number' && !isNaN(now) ? now.toFixed(0) : '0';
        console.log(`[SYNC] ${timeStr}ms â–¶ viseme ${id} (ç†æƒ³ ${t}ms)`);
      }
      if (!audio.paused && idx < visemeTimeline.length) requestAnimationFrame(tick);
    }
    requestAnimationFrame(tick);
  }

  // è¯­éŸ³åˆæˆ
  async function speak() {
    if (!text.value.trim()) return;
    try {
      isProcessing.value = true;
      const audioBlob = await synthesizeSpeech(
        ssml.value || text.value,
        selectedVoice.value,
        Boolean(ssml.value),
      );
      audioUrl.value = URL.createObjectURL(audioBlob);
    } catch (error) {
      console.error('Failed to synthesize speech:', error);
      alert(t('animate.synthesisError'));
    } finally {
      isProcessing.value = false;
    }
  }

  return {
    isProcessing,
    audioUrl,
    animationTimer,
    visemeTimeline,
    onAnimate,
    startTimelineAnimation,
    handleViseme,
    syncVisemeWithAudio,
    speak
  };
}
