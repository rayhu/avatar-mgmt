/**
 * åç«¯ Azure TTS API å®¢æˆ·ç«¯
 * é€šè¿‡åç«¯ä»£ç†è°ƒç”¨ Azure TTSï¼Œé¿å…åœ¨å‰ç«¯æš´éœ² Azure key
 */

const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 'http://localhost:3000';

export interface VoiceOption {
  name: string;
  label: string;
  styles?: string[];
}

// å¯ç”¨çš„ä¸­æ–‡è¯­éŸ³åˆ—è¡¨ï¼ˆé™æ€æ•°æ®ï¼Œé¿å…é¢‘ç¹è¯·æ±‚ï¼‰
export const availableVoices: VoiceOption[] = [
  { name: 'zh-CN-XiaoxiaoNeural', label: 'æ™“æ™“ (å¥³å£°)' },
  { name: 'zh-CN-YunxiNeural', label: 'äº‘å¸Œ (ç”·å£°)' },
  { name: 'zh-CN-YunyangNeural', label: 'äº‘æ‰¬ (ç”·å£°)' },
  { name: 'zh-CN-XiaochenNeural', label: 'æ™“è¾° (å¥³å£°)' },
  { name: 'zh-CN-XiaohanNeural', label: 'æ™“æ¶µ (å¥³å£°)' },
  { name: 'zh-CN-XiaomoNeural', label: 'æ™“å¢¨ (å¥³å£°)' },
  { name: 'zh-CN-XiaoxuanNeural', label: 'æ™“è± (å¥³å£°)' },
  { name: 'zh-CN-XiaoyanNeural', label: 'æ™“é¢œ (å¥³å£°)' },
  { name: 'zh-CN-XiaoyouNeural', label: 'æ™“æ‚  (å¥³å£°)' },
  { name: 'zh-CN-YunjianNeural', label: 'äº‘å¥ (ç”·å£°)' },
  { name: 'zh-CN-YunxiaNeural', label: 'äº‘å¤ (ç”·å£°)' },
  { name: 'zh-CN-YunzeNeural', label: 'äº‘æ³½ (ç”·å£°)' },
];

/**
 * è¯­éŸ³åˆæˆï¼ˆé€šè¿‡åç«¯ Azure TTS APIï¼‰
 * å‚è€ƒå‰ç«¯ azureTTS.ts çš„å®ç°ï¼Œæ”¯æŒæ–‡æœ¬å’Œ SSML è¾“å…¥
 * @param content è¦åˆæˆçš„å†…å®¹ï¼ˆæ–‡æœ¬æˆ– SSMLï¼‰
 * @param voice è¯­éŸ³åç§°
 * @param isSSML æ˜¯å¦ä¸º SSML æ ¼å¼
 * @param onViseme å£å‹å›è°ƒå‡½æ•°ï¼ˆåç«¯æš‚ä¸æ”¯æŒï¼Œè¿”å›ç©ºå®ç°ï¼‰
 * @returns éŸ³é¢‘ Blob
 */
export async function synthesizeSpeech(
  content: string,
  voice: string = 'zh-CN-XiaoxiaoNeural',
  isSSML: boolean = false,
  onViseme?: (id: number, timeMs: number, animation?: string) => void,
): Promise<Blob> {
  const url = `${API_BASE_URL}/api/azure-tts`;
  
  console.log('ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆè¯·æ±‚:', {
    url,
    voice,
    isSSML,
    contentLength: content.length,
    contentPreview: content.slice(0, 100) + '...'
  });
  
  // åç«¯æš‚ä¸æ”¯æŒ visemeï¼Œè¿”å›ç©ºå®ç°
  if (onViseme) {
    console.warn('âš ï¸ Viseme callback is not supported in backend mode');
  }
  
  try {
    // å¤„ç† SSML æ ¼å¼ï¼Œå‚è€ƒå‰ç«¯ azureTTS.ts çš„é€»è¾‘
    let processedContent = content;
    if (isSSML) {
      // ä¸å¯é çš„ SSML ç›´æ¥é‡æ„ä¸ºæ ‡å‡†æ ¼å¼
      // 0st æ˜¯æ— æ•ˆçš„ï¼Œéœ€è¦è½¬æ¢ä¸º +0st
      processedContent = content.replace(/pitch="0st"/g, 'pitch="+0st"');

      const inner = content
        .replace(/<speak[\s\S]*?>/, '')
        .replace(/<\/speak>/, '')
        .trim();

      const hasVoice = /<voice[\s>]/i.test(inner);
      let processed = inner;
      if (hasVoice) {
        const m = inner.match(/<voice[^>]*name=["']([^"']+)["']/i);
        const detected = m?.[1];
        if (detected && detected !== voice) {
          console.warn(
            `[BackendAzureTTS] LLM returned voice "${detected}" which differs from selected "${voice}". Using LLM-specified voice.`,
          );
        }
      } else {
        processed = `<voice name="${voice}">${inner}</voice>`;
      }
      const wrapped = processed;
      processedContent = `<speak version="1.0" xml:lang="zh-CN" xmlns="http://www.w3.org/2001/10/synthesis" xmlns:mstts="http://www.w3.org/2001/mstts">${wrapped}</speak>`;
    } else {
      // çº¯æ–‡æœ¬ï¼ŒåŒ…è£…ä¸ºç®€å• SSML
      processedContent = `<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="zh-CN">
  <voice name="${voice}">
    ${content}
  </voice>
</speak>`;
    }

    console.log('ğŸ“¡ å‘é€è¯·æ±‚åˆ°:', url);
    console.log('ğŸ“ å¤„ç†åçš„å†…å®¹:', processedContent);
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ 
        ssml: processedContent, 
        voice 
      }),
    });

    console.log('ğŸ“¥ æ”¶åˆ°å“åº”:', {
      status: response.status,
      statusText: response.statusText,
      ok: response.ok,
      headers: Object.fromEntries(response.headers.entries())
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      console.error('âŒ è¯­éŸ³åˆæˆå¤±è´¥:', {
        status: response.status,
        statusText: response.statusText,
        errorData
      });
      throw new Error(`Speech synthesis failed: ${errorData.error || response.statusText}`);
    }

    const blob = await response.blob();
    console.log('âœ… è¯­éŸ³åˆæˆæˆåŠŸ:', {
      blobSize: blob.size,
      blobType: blob.type
    });
    
    return blob;
  } catch (error) {
    const err = error as Error;
    console.error('âŒ è¯­éŸ³åˆæˆè¯·æ±‚å¤±è´¥:', {
      error: err.message,
      errorType: err.constructor.name,
      url,
      voice,
      contentLength: content.length
    });
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œé”™è¯¯
    if (err.name === 'TypeError' && err.message.includes('Failed to fetch')) {
      console.error('ğŸŒ ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œå¯èƒ½çš„åŸå› :');
      console.error('   - åç«¯æœåŠ¡å™¨æœªå¯åŠ¨');
      console.error('   - API_BASE_URL é…ç½®é”™è¯¯:', API_BASE_URL);
      console.error('   - CORS é…ç½®é—®é¢˜');
      console.error('   - ç½‘ç»œè¿æ¥é—®é¢˜');
    }
    
    throw error;
  }
}

/**
 * ä¸€é”®è¯­éŸ³åˆæˆï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰
 * @param text è¦è½¬æ¢çš„æ–‡æœ¬
 * @param voice è¯­éŸ³åç§°
 * @returns éŸ³é¢‘ Blob
 */
export async function textToSpeech(text: string, voice: string = 'zh-CN-XiaoxiaoNeural'): Promise<Blob> {
  console.log('ğŸ¯ å¼€å§‹æ–‡æœ¬è½¬è¯­éŸ³:', {
    text: text.slice(0, 50) + (text.length > 50 ? '...' : ''),
    voice,
    textLength: text.length,
    apiBaseUrl: API_BASE_URL
  });
  
  try {
    // ç›´æ¥è°ƒç”¨ synthesizeSpeechï¼Œä¼ å…¥çº¯æ–‡æœ¬
    const audioBlob = await synthesizeSpeech(text, voice, false);
    
    console.log('ğŸ‰ æ–‡æœ¬è½¬è¯­éŸ³å®Œæˆ:', {
      blobSize: audioBlob.size,
      blobType: audioBlob.type
    });
    
    return audioBlob;
  } catch (error) {
    const err = error as Error;
    console.error('âŒ æ–‡æœ¬è½¬è¯­éŸ³å¤±è´¥:', {
      error: err.message,
      errorType: err.constructor.name,
      text: text.slice(0, 50) + (text.length > 50 ? '...' : ''),
      voice,
      apiBaseUrl: API_BASE_URL
    });
    throw error;
  }
}

/**
 * æ’­æ”¾éŸ³é¢‘
 * @param audioBlob éŸ³é¢‘ Blob
 */
export function playAudio(audioBlob: Blob): void {
  const audioUrl = URL.createObjectURL(audioBlob);
  const audio = new Audio(audioUrl);
  
  audio.onended = () => {
    URL.revokeObjectURL(audioUrl); // æ¸…ç†å†…å­˜
  };
  
  audio.play().catch(error => {
    console.error('Audio playback error:', error);
    URL.revokeObjectURL(audioUrl);
  });
}

/**
 * ä¸‹è½½éŸ³é¢‘
 * @param audioBlob éŸ³é¢‘ Blob
 * @param filename æ–‡ä»¶å
 */
export function downloadAudio(audioBlob: Blob, filename: string = 'speech.mp3'): void {
  const url = URL.createObjectURL(audioBlob);
  const a = document.createElement('a');
  a.href = url;
  a.download = filename;
  document.body.appendChild(a);
  a.click();
  document.body.removeChild(a);
  URL.revokeObjectURL(url);
}

/**
 * è·å–å¯ç”¨çš„è¯­éŸ³åˆ—è¡¨
 * @returns Promise<VoiceOption[]> è¯­éŸ³åˆ—è¡¨
 */
export async function fetchVoices(): Promise<VoiceOption[]> {
  // åç«¯æ¨¡å¼ä¸‹ç›´æ¥è¿”å›é™æ€åˆ—è¡¨
  return availableVoices;
}

/**
 * æ£€æŸ¥åç«¯ Azure TTS æœåŠ¡æ˜¯å¦å¯ç”¨
 * @returns Promise<boolean>
 */
export async function checkBackendAvailability(): Promise<boolean> {
  try {
    const response = await fetch(`${API_BASE_URL}/health`);
    return response.ok;
  } catch (error) {
    console.warn('Backend Azure TTS not available:', error);
    return false;
  }
}
